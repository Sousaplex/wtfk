#!/usr/bin/env python3
"""
WTFK (What The Foreign Key) - Schema Analyzer V5 (Corrected)
"""

import argparse
import sys
import os
import json
from pathlib import Path
from datetime import datetime

try:
    from langchain.prompts import PromptTemplate
    from langchain_google_genai import ChatGoogleGenerativeAI
    from dotenv import load_dotenv
    import click
except ImportError:
    print("Error: Missing required packages. Please run 'pip install -r requirements.txt'")
    sys.exit(1)

class SchemaAnalyzer:
    def __init__(self, api_key=None, model_name=None, settings_file="settings.json"):
        self.settings = self.load_settings(settings_file)
        if model_name:
            self.settings['model']['name'] = model_name
        
        load_dotenv()
        self.api_key = api_key or os.getenv('GOOGLE_API_KEY')
        if not self.api_key:
            print("FATAL ERROR: Google API Key not found.")
            sys.exit(1)
        
        try:
            self.llm = ChatGoogleGenerativeAI(
                model=self.settings['model']['name'],
                google_api_key=self.api_key,
                temperature=self.settings['model'].get('temperature', 0.1),
                max_output_tokens=self.settings['model'].get('max_output_tokens', 8192)
            )
        except Exception as e:
            print(f"FATAL ERROR: Could not initialize Gemini: {e}")
            sys.exit(1)
    
    def load_settings(self, settings_file):
        try:
            with open(settings_file, 'r') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return {"paths": {"prompts_dir": "prompts"}}

    def load_prompt_template(self, prompt_file):
        try:
            with open(prompt_file, 'r') as f:
                template_content = f.read()
            # This is the corrected, simplified logic.
            return PromptTemplate.from_template(template_content)
        except Exception as e:
            print(f"FATAL ERROR: Could not load prompt template {prompt_file}: {e}")
            sys.exit(1)

    def build_compliance_context(self, frameworks):
        if not frameworks: return ""
        context_parts = ["\n\n--- START COMPLIANCE ANALYSIS CONTEXT ---\n"]
        base_dir = self.settings['paths'].get('compliance_dir', 'compliance')
        snippets_dir = self.settings['paths'].get('compliance_snippets_dir', 'prompts/compliance_snippets')
        for framework in frameworks:
            context_parts.append(f"## Analysis Lens: {framework.upper()}\n")
            try:
                context_parts.append(Path(snippets_dir, f"{framework}.txt").read_text())
                context_parts.append(f"\n### Framework Definition: {framework.upper()}\n")
                context_parts.append(Path(base_dir, f"{framework}.md").read_text())
            except FileNotFoundError as e:
                print(f"Warning: Missing file for framework '{framework}': {e}")
            context_parts.append("\n---\n")
        context_parts.append("--- END COMPLIANCE ANALYSIS CONTEXT ---\n\n")
        return "".join(context_parts)

    def analyze_schema(self, prompt_value):
        print("Analyzing schema with Gemini...")
        try:
            response = self.llm.invoke(prompt_value)
            return response.content, response.response_metadata.get('usage_metadata', {})
        except Exception as e:
            print(f"FATAL ERROR during analysis: {e}")
            sys.exit(1)
    
    def save_report(self, analysis_result, output_file, source_file):
        project_name = self.settings.get('project', {}).get('name', 'Database Schema Analysis')
        report = f"# {project_name} Report\n\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n**Source Context:** {source_file}\n**Analyzer:** {self.settings['model']['name']}\n\n---\n\n{analysis_result}\n\n---\n\n*Report generated by WTFK*\n"
        Path(output_file).write_text(report, encoding='utf-8')
        print(f"Analysis report saved to: {output_file}")

def analyze_schema_with_compliance(context_file, output_file=None, frameworks=None, settings_file="settings.json", api_key=None, model=None, prompt_file=None):
    analyzer = SchemaAnalyzer(api_key=api_key, model_name=model, settings_file=settings_file)
    
    prompt_file = prompt_file or analyzer.settings.get('analysis', {}).get('default_prompt', 'prompts/schema_analysis.txt')
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = Path(output_file or Path(analyzer.settings['paths'].get('output_dir', 'output')) / f"schema_analysis_{timestamp}.md")
    output_path.parent.mkdir(exist_ok=True)

    click.echo(click.style("\n--- Verbose Analysis Step ---", bold=True))
    
    compliance_context = analyzer.build_compliance_context(frameworks or [])
    click.echo(f"1. Compliance context built. Length: {len(compliance_context)} chars.")

    try:
        with open(context_file, 'r') as f:
            full_context_json = json.load(f)
        schema_content_str = json.dumps(full_context_json, indent=2)
        click.echo(f"2. Loaded and serialized context JSON. Length: {len(schema_content_str)} chars.")
    except Exception as e:
        print(f"FATAL ERROR: Could not load or parse context file {context_file}: {e}")
        sys.exit(1)

    prompt_template = analyzer.load_prompt_template(prompt_file)
    click.echo(f"3. Prompt template loaded. Expects variables: {prompt_template.input_variables}")

    prompt_value = prompt_template.format_prompt(
        schema_content=schema_content_str,
        compliance_context=compliance_context
    )
    full_prompt_text = prompt_value.to_string()
    click.echo("4. Prompt formatted with context.")

    click.echo("5. Running safeguard check...")
    if "{schema_content}" in full_prompt_text or "{compliance_context}" in full_prompt_text:
        print("\nFATAL ERROR: Prompt variables were not replaced correctly. Aborting.\n")
        sys.exit(1)
    click.echo(click.style("   âœ… Safeguard passed.", fg='green'))
    
    log_file = Path("logs") / f"schema_analysis_prompt_{timestamp}.log"
    log_file.parent.mkdir(exist_ok=True)
    log_file.write_text(f"--- PROMPT SENT TO LLM ---\n\n{full_prompt_text}\n\n--- END PROMPT ---", encoding='utf-8')
    click.echo(f"6. Full prompt saved to: {log_file}")

    analysis_result, usage_metadata = analyzer.analyze_schema(prompt_value)
    
    if usage_metadata:
        click.echo(click.style("\nðŸ“Š LLM Token Usage (Analysis):", fg='blue', bold=True))
        click.echo(f"   - Input Tokens: {usage_metadata.get('prompt_token_count', 0):,}")
        click.echo(f"   - Output Tokens: {usage_metadata.get('candidates_token_count', 0):,}")
        click.echo(f"   - Total Tokens: {usage_metadata.get('total_token_count', 0):,}")

    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"\n\n--- RESPONSE FROM LLM ---\n\n{analysis_result}\n\n--- END RESPONSE ---")

    analyzer.save_report(analysis_result, output_path, str(context_file))
    return output_path

def main():
    parser = argparse.ArgumentParser(description="Analyze a database schema using a generated context file.")
    parser.add_argument("context_file", help="Path to the generated context JSON file")
    parser.add_argument("-o", "--output", help="Output markdown file path")
    parser.add_argument("--frameworks", nargs='+', help='Compliance frameworks to analyze.')
    parser.add_argument("-s", "--settings", default="settings.json", help="Path to settings.json file")
    args = parser.parse_args()
    
    if not analyze_schema_with_compliance(
        context_file=args.context_file,
        output_file=args.output,
        frameworks=args.frameworks,
        settings_file=args.settings
    ):
        sys.exit(1)

if __name__ == "__main__":
    main()